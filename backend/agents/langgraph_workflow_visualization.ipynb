{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🚀 LangGraph Agent Workflow Visualization\n",
        "\n",
        "This notebook showcases the beautiful architecture of our Interview Preparation Agent built with LangGraph.\n",
        "\n",
        "## 🎯 Agent Overview\n",
        "\n",
        "Our agent consists of three main workflows:\n",
        "1. **Interview Challenge Generation** - Creates MCQ questions\n",
        "2. **Scenario Challenge Generation** - Creates open-ended scenarios\n",
        "3. **Answer Evaluation** - Evaluates user responses with AI feedback\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔄 Complete Agent Architecture\n",
        "\n",
        "Here's the high-level view of our entire agent system:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📝 MCQ Generation Workflow\n",
        "\n",
        "Deep dive into the Interview Challenge (MCQ) generation process:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎭 Scenario Generation Workflow\n",
        "\n",
        "Detailed view of the Scenario Challenge generation process:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔍 Answer Evaluation Workflow\n",
        "\n",
        "The intelligent evaluation system that provides AI-powered feedback:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🏗️ State Management Architecture\n",
        "\n",
        "How LangGraph manages state across all workflows:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎨 Agent Features & Benefits\n",
        "\n",
        "### ✨ Key Features:\n",
        "- **🔄 Structured Workflows**: Clear separation of concerns with LangGraph\n",
        "- **🎯 State Management**: Proper data flow and state persistence\n",
        "- **🤖 AI-Powered**: OpenAI integration for intelligent content generation\n",
        "- **📊 Smart Evaluation**: Comprehensive answer assessment with scoring\n",
        "- **🛡️ Error Handling**: Robust validation and error management\n",
        "- **🔧 Extensible**: Easy to add new nodes and workflows\n",
        "\n",
        "### 🚀 Benefits:\n",
        "- **Maintainability**: Clean, modular architecture\n",
        "- **Scalability**: Easy to extend with new challenge types\n",
        "- **Reliability**: Proper state management and error handling\n",
        "- **Observability**: LangGraph Studio integration for monitoring\n",
        "- **Performance**: Efficient workflow execution\n",
        "\n",
        "### 🔮 Future Enhancements:\n",
        "- **Multi-step evaluation** with intermediate feedback\n",
        "- **Adaptive difficulty** based on user performance\n",
        "- **Personalized content** generation\n",
        "- **Multi-modal support** (text, images, code)\n",
        "- **Real-time collaboration** features\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎯 Usage Examples\n",
        "\n",
        "Here's how to use the agent in your application:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage of the LangGraph agent\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "\n",
        "from agents.ai_generator_agentic import (\n",
        "    generate_interview_challenges,\n",
        "    generate_scenario_challenge,\n",
        "    evaluate_scenario_answer\n",
        ")\n",
        "\n",
        "# Generate interview challenges\n",
        "print(\"🎯 Generating Interview Challenges...\")\n",
        "mcq_challenges = generate_interview_challenges(\n",
        "    topic=\"Machine Learning\",\n",
        "    difficulty=\"medium\",\n",
        "    num_questions=3\n",
        ")\n",
        "\n",
        "print(f\"✅ Generated {len(mcq_challenges)} challenges\")\n",
        "for i, challenge in enumerate(mcq_challenges):\n",
        "    print(f\"  {i+1}. {challenge['title'][:60]}...\")\n",
        "\n",
        "# Generate scenario challenge\n",
        "print(\"\\n🎭 Generating Scenario Challenge...\")\n",
        "scenario = generate_scenario_challenge(\n",
        "    topic=\"Data Science\",\n",
        "    difficulty=\"hard\",\n",
        "    num_questions=2\n",
        ")\n",
        "\n",
        "print(f\"✅ Generated scenario: {scenario['title'][:60]}...\")\n",
        "\n",
        "# Evaluate an answer\n",
        "print(\"\\n🔍 Evaluating Answer...\")\n",
        "evaluation = evaluate_scenario_answer(\n",
        "    user_answer=\"I would use cross-validation and feature selection\",\n",
        "    correct_answer=\"Use cross-validation, feature selection, and regularization\",\n",
        "    scenario_title=\"How to improve model performance\",\n",
        "    questions='[{\"prompt\": \"What techniques would you use?\"}]'\n",
        ")\n",
        "\n",
        "print(f\"✅ Score: {evaluation['score']}/100\")\n",
        "print(f\"📝 Feedback: {evaluation['feedback'][:80]}...\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
